{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning example with stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/neurogym/neurogym/blob/master/examples/example_neurogym_rl.ipynb)\n",
    "\n",
    "NeuroGym is a toolkit that allows training any network model on many established neuroscience tasks techniques such as standard Supervised  Learning or Reinforcement Learning (RL). In this notebook we will use RL to train an LSTM network on the classical Random Dots Motion (RDM) task (Britten et al. 1992).\n",
    "\n",
    "We first show how to install the relevant toolboxes. We then show how build the task of interest (in the example the RDM task), wrapp it with the pass-reward wrapper in one line and visualize the structure of the final task. Finally we train an LSTM network on the task using the A2C algorithm [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) implemented in the [stable-baselines](https://github.com/hill-a/stable-baselines) toolbox, and plot the results.\n",
    "\n",
    "It is straightforward to change the code to train a network on any other available task or using a different RL algorithm (e.g. ACER, PPO2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
=======
    "### Instalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "# Install gym\n",
    "! pip install gym\n",
    "# Install neurogym\n",
    "! git clone https://github.com/gyyang/neurogym.git\n",
    "%cd neurogym/\n",
    "! pip install -e .\n",
    "# Install stable-baselines\n",
    "! pip install --upgrade stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> f81454b516af464958e94eb1b3beca38297e12ac
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we build the Random Dots Motion task, specifying the duration of each trial period (fixation, stimulus, decision) and wrapp it with the pass-reward wrapper which appends the previous reward to the observation. We then plot the structure of the task in a figure that shows: \n",
    "1. The observations received by the agent (top panel). \n",
    "2. The actions taken by a random agent and the correct action at each timestep (second panel).\n",
    "3. The rewards provided by the environment at each timestep (third panel).\n",
    "4. The performance of the agent at each trial (bottom panel)."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> f81454b516af464958e94eb1b3beca38297e12ac
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[0;32m<ipython-input-1-339f97dcf4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneurogym\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mngym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneurogym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpass_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
=======
      "\u001b[0;32m<ipython-input-2-339f97dcf4f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mneurogym\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mngym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneurogym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpass_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
>>>>>>> f81454b516af464958e94eb1b3beca38297e12ac
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import neurogym as ngym\n",
    "from neurogym.wrappers import pass_reward\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Task name\n",
    "name = 'PerceptualDecisionMakingDelayResponse-v0'\n",
    "# task specification (here we only specify the duration of the different trial periods)\n",
    "timing = {'fixation': ('constant', 100),\n",
    "          'stimulus': ('constant', 300),\n",
    "          'delay':(0, 100, 300),\n",
    "          'decision': ('constant', 200)}\n",
    "rewards = {'abort': -0.1, 'correct': +1., 'fail': 0.}\n",
    "kwargs = {'dt': 100, 'timing': timing, 'rewards': rewards}\n",
    "# build task\n",
    "env = gym.make(name, **kwargs)\n",
    "# print task properties\n",
    "print(env)\n",
    "# wrapp task with pass-reward wrapper\n",
    "# env = pass_reward.PassReward(env)\n",
    "# plot example trials with random agent\n",
    "data = ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=50, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2']) # , 'Previous reward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from stable_baselines.common.policies import LstmPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import A2C  # ACER, PPO2\n",
    "warnings.filterwarnings('default')\n",
    "num_steps = 1000000\n",
    "num_instances = 3\n",
    "mean_perf = []\n",
    "perf_list = []\n",
    "for ind in range(num_instances):\n",
    "  env = gym.make(name, **kwargs)\n",
    "  # Optional: PPO2 requires a vectorized environment to run\n",
    "  # the env is now wrapped automatically when passing it to the constructor\n",
    "  env = DummyVecEnv([lambda: env])\n",
    "\n",
    "  model = A2C(LstmPolicy, env, verbose=1, policy_kwargs={'feature_extraction':\"mlp\"})\n",
    "  model.learn(total_timesteps=num_steps, log_interval=10000) # TODO: check minimum total_timesteps value with which the networks learn when fail=0\n",
    "  data = ngym.utils.plotting.run_env(env, num_trials=1000, model=model)\n",
    "  perf = np.array(data['perf'])\n",
    "  mean_perf.append(np.mean(perf[perf != -1]))\n",
    "  env.close()\n",
    "perf_list.append(mean_perf)\n",
    "print(perf_list)\n",
    "print(np.mean(mean_perf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(name, **kwargs)\n",
    "# print task properties\n",
    "print(env)\n",
    "# wrapp task with pass-reward wrapper\n",
    "# env = pass_reward.PassReward(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# plot example trials with random agent\n",
    "ngym.utils.plot_env(env, fig_kwargs={'figsize': (12, 12)}, num_steps=50, ob_traces=['Fixation cue', 'Stim 1', 'Stim 2'], model=model)  # , 'Previous reward'\n",
    "data = ngym.utils.plotting.run_env(env, num_trials=1000, model=model)\n",
    "    # data = {\n",
    "    #     'ob': np.array(observations).astype(np.float), observations received by network\n",
    "    #     'ob_cum': np.array(ob_cum).astype(np.float),\n",
    "    #     'rewards': rewards,\n",
    "    #     'actions': actions,\n",
    "    #     'perf': perf,\n",
    "    #     'actions_end_of_trial': actions_end_of_trial,\n",
    "    #     'gt': gt,\n",
    "    #     'states': states\n",
    "    # }\n",
    "# select indx where data['perf'] is diff. from -1 and average"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
